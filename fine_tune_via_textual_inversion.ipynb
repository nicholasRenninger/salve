{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1IklnbSG9FV"
      },
      "source": [
        "# Teach StableDiffusion new concepts via Textual Inversion\n",
        "\n",
        "**Authors:** Ian Stenbit, [lukewood](https://lukewood.xyz)<br>\n",
        "**Date created:** 2022/12/09<br>\n",
        "**Last modified:** 2022/12/09<br>\n",
        "**Description:** Learning new visual concepts with KerasCV's StableDiffusion implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJEgxNUqG9FZ"
      },
      "source": [
        "## Textual Inversion\n",
        "\n",
        "Since its release, StableDiffusion has quickly become a favorite amongst\n",
        "the generative machine learning community.\n",
        "The high volume of traffic has led to open source contributed improvements,\n",
        "heavy prompt engineering, and even the invention of novel algorithms.\n",
        "\n",
        "Perhaps the most impressive new algorithm being used is\n",
        "[Textual Inversion](https://github.com/rinongal/textual_inversion), presented in\n",
        "[_An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion_](https://textual-inversion.github.io/).\n",
        "\n",
        "Textual Inversion is the process of teaching an image generator a specific visual concept\n",
        "through the use of fine-tuning. In the diagram below, you can see an\n",
        "example of this process where the authors teach the model new concepts, calling them\n",
        "\"S_*\".\n",
        "\n",
        "![https://i.imgur.com/KqEeBsM.jpg](https://i.imgur.com/KqEeBsM.jpg)\n",
        "\n",
        "Conceptually, textual inversion works by learning a token embedding for a new text\n",
        "token, keeping the remaining components of StableDiffusion frozen.\n",
        "\n",
        "This guide shows you how to fine-tune the StableDiffusion model shipped in KerasCV\n",
        "using the Textual-Inversion algorithm.  By the end of the guide, you will be able to\n",
        "write the \"Gandalf the Gray as a <my-funny-cat-token>\".\n",
        "\n",
        "![https://i.imgur.com/rcb1Yfx.png](https://i.imgur.com/rcb1Yfx.png)\n",
        "\n",
        "\n",
        "First, let's import the packages we need, and create a\n",
        "StableDiffusion instance so we can use some of its subcomponents for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAcvuTFXG9Fa"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/keras-team/keras-cv.git\n",
        "!pip install -q tensorflow==2.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GWfGOVNG9Fb"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "import keras_cv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras_cv import layers as cv_layers\n",
        "from keras_cv.models.stable_diffusion import NoiseScheduler\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "stable_diffusion = keras_cv.models.StableDiffusion()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7HsOSN6G9Fc"
      },
      "source": [
        "Next, let's define a visualization utility to show off the generated images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YMRNN08G9Fc"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_images(images):\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    for i in range(len(images)):\n",
        "        ax = plt.subplot(1, len(images), i + 1)\n",
        "        plt.imshow(images[i])\n",
        "        plt.axis(\"off\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDLSrHexG9Fd"
      },
      "source": [
        "## Assembling a text-image pair dataset\n",
        "\n",
        "In order to train the embedding of our new token, we first must assemble a dataset\n",
        "consisting of text-image pairs.\n",
        "Each sample from the dataset must contain an image of the concept we are teaching\n",
        "StableDiffusion, as well as a caption accurately representing the content of the image.\n",
        "In this tutorial, we will teach StableDiffusion the concept of Luke and Ian's GitHub\n",
        "avatars:\n",
        "\n",
        "![gh-avatars](https://i.imgur.com/WyEHDIR.jpg)\n",
        "\n",
        "First, let's construct an image dataset of cat dolls:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Obiqig0GG9Fe"
      },
      "outputs": [],
      "source": [
        "\n",
        "def assemble_image_dataset(urls):\n",
        "    # Fetch all remote files\n",
        "    files = [tf.keras.utils.get_file(origin=url) for url in urls]\n",
        "\n",
        "    # Resize images\n",
        "    resize = keras.layers.Resizing(height=512, width=512, crop_to_aspect_ratio=True)\n",
        "    images = [keras.utils.load_img(img) for img in files]\n",
        "    images = [keras.utils.img_to_array(img) for img in images]\n",
        "    images = np.array([resize(img) for img in images])\n",
        "\n",
        "    # The StableDiffusion image encoder requires images to be normalized to the\n",
        "    # [-1, 1] pixel value range\n",
        "    images = images / 127.5 - 1\n",
        "\n",
        "    # Create the tf.data.Dataset\n",
        "    image_dataset = tf.data.Dataset.from_tensor_slices(images)\n",
        "\n",
        "    # Shuffle and introduce random noise\n",
        "    image_dataset = image_dataset.shuffle(50, reshuffle_each_iteration=True)\n",
        "    image_dataset = image_dataset.map(\n",
        "        cv_layers.RandomCropAndResize(\n",
        "            target_size=(512, 512),\n",
        "            crop_area_factor=(0.8, 1.0),\n",
        "            aspect_ratio_factor=(1.0, 1.0),\n",
        "        ),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    )\n",
        "    image_dataset = image_dataset.map(\n",
        "        cv_layers.RandomFlip(mode=\"horizontal\"),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    )\n",
        "    return image_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s02PZ00sG9Fe"
      },
      "source": [
        "Next, we assemble a text dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS8b1scjG9Ff"
      },
      "outputs": [],
      "source": [
        "MAX_PROMPT_LENGTH = 77\n",
        "placeholder_token = \"<my-funny-cat-token>\"\n",
        "\n",
        "\n",
        "def pad_embedding(embedding):\n",
        "    return embedding + (\n",
        "        [stable_diffusion.tokenizer.end_of_text] * (MAX_PROMPT_LENGTH - len(embedding))\n",
        "    )\n",
        "\n",
        "\n",
        "stable_diffusion.tokenizer.add_tokens(placeholder_token)\n",
        "\n",
        "\n",
        "def assemble_text_dataset(prompts):\n",
        "    prompts = [prompt.format(placeholder_token) for prompt in prompts]\n",
        "    embeddings = [stable_diffusion.tokenizer.encode(prompt) for prompt in prompts]\n",
        "    embeddings = [np.array(pad_embedding(embedding)) for embedding in embeddings]\n",
        "    text_dataset = tf.data.Dataset.from_tensor_slices(embeddings)\n",
        "    text_dataset = text_dataset.shuffle(100, reshuffle_each_iteration=True)\n",
        "    return text_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCeRl-2GG9Fg"
      },
      "source": [
        "Finally, we zip our datasets together to produce a text-image pair dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1385rUlKG9Fg"
      },
      "outputs": [],
      "source": [
        "\n",
        "def assemble_dataset(urls, prompts):\n",
        "    image_dataset = assemble_image_dataset(urls)\n",
        "    text_dataset = assemble_text_dataset(prompts)\n",
        "    # the image dataset is quite short, so we repeat it to match the length of the\n",
        "    # text prompt dataset\n",
        "    image_dataset = image_dataset.repeat()\n",
        "    # we use the text prompt dataset to determine the length of the dataset.  Due to\n",
        "    # the fact that there are relatively few prompts we repeat the dataset 5 times.\n",
        "    # we have found that this anecdotally improves results.\n",
        "    text_dataset = text_dataset.repeat(5)\n",
        "    return tf.data.Dataset.zip((image_dataset, text_dataset))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOVCy5XsG9Fg"
      },
      "source": [
        "In order to ensure our prompts are descriptive, we use extremely generic prompts.\n",
        "\n",
        "Let's try this out with some sample images and prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYENlDTCG9Fh"
      },
      "outputs": [],
      "source": [
        "train_ds = assemble_dataset(\n",
        "    urls=[\n",
        "        \"https://i.imgur.com/VIedH1X.jpg\",\n",
        "        \"https://i.imgur.com/eBw13hE.png\",\n",
        "        \"https://i.imgur.com/oJ3rSg7.png\",\n",
        "        \"https://i.imgur.com/5mCL6Df.jpg\",\n",
        "        \"https://i.imgur.com/4Q6WWyI.jpg\",\n",
        "    ],\n",
        "    prompts=[\n",
        "        \"a photo of a {}\",\n",
        "        \"a rendering of a {}\",\n",
        "        \"a cropped photo of the {}\",\n",
        "        \"the photo of a {}\",\n",
        "        \"a photo of a clean {}\",\n",
        "        \"a dark photo of the {}\",\n",
        "        \"a photo of my {}\",\n",
        "        \"a photo of the cool {}\",\n",
        "        \"a close-up photo of a {}\",\n",
        "        \"a bright photo of the {}\",\n",
        "        \"a cropped photo of a {}\",\n",
        "        \"a photo of the {}\",\n",
        "        \"a good photo of the {}\",\n",
        "        \"a photo of one {}\",\n",
        "        \"a close-up photo of the {}\",\n",
        "        \"a rendition of the {}\",\n",
        "        \"a photo of the clean {}\",\n",
        "        \"a rendition of a {}\",\n",
        "        \"a photo of a nice {}\",\n",
        "        \"a good photo of a {}\",\n",
        "        \"a photo of the nice {}\",\n",
        "        \"a photo of the small {}\",\n",
        "        \"a photo of the weird {}\",\n",
        "        \"a photo of the large {}\",\n",
        "        \"a photo of a cool {}\",\n",
        "        \"a photo of a small {}\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpGfDHR3G9Fh"
      },
      "source": [
        "## On the importance of prompt accuracy\n",
        "\n",
        "During our first attempt at writing this guide we included images of groups of these cat\n",
        "dolls in our dataset but continued to use the generic prompts listed above.\n",
        "Our results were anecdotally poor. For example, here's cat doll gandalf using this method:\n",
        "\n",
        "![mediocre-wizard](https://i.imgur.com/Thq7XOu.jpg)\n",
        "\n",
        "It's conceptually close, but it isn't as great as it can be.\n",
        "\n",
        "In order to remedy this, we began experimenting with splitting our images into images of\n",
        "singular cat dolls and groups of cat dolls.\n",
        "Following this split, we came up with new prompts for the group shots.\n",
        "\n",
        "Training on text-image pairs that accurately represent the content boosted the quality\n",
        "of our results *substantially*.  This speaks to the importance of prompt accuracy.\n",
        "\n",
        "In addition to separating the images into singular and group images, we also remove some\n",
        "inaccurate prompts; such as \"a dark photo of the {}\"\n",
        "\n",
        "Keeping this in mind, we assemble our final training dataset below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KA9lFwzzG9Fh"
      },
      "outputs": [],
      "source": [
        "single_ds = assemble_dataset(\n",
        "    urls=[\n",
        "        \"https://i.imgur.com/VIedH1X.jpg\",\n",
        "        \"https://i.imgur.com/eBw13hE.png\",\n",
        "        \"https://i.imgur.com/oJ3rSg7.png\",\n",
        "        \"https://i.imgur.com/5mCL6Df.jpg\",\n",
        "        \"https://i.imgur.com/4Q6WWyI.jpg\",\n",
        "    ],\n",
        "    prompts=[\n",
        "        \"a photo of a {}\",\n",
        "        \"a rendering of a {}\",\n",
        "        \"a cropped photo of the {}\",\n",
        "        \"the photo of a {}\",\n",
        "        \"a photo of a clean {}\",\n",
        "        \"a photo of my {}\",\n",
        "        \"a photo of the cool {}\",\n",
        "        \"a close-up photo of a {}\",\n",
        "        \"a bright photo of the {}\",\n",
        "        \"a cropped photo of a {}\",\n",
        "        \"a photo of the {}\",\n",
        "        \"a good photo of the {}\",\n",
        "        \"a photo of one {}\",\n",
        "        \"a close-up photo of the {}\",\n",
        "        \"a rendition of the {}\",\n",
        "        \"a photo of the clean {}\",\n",
        "        \"a rendition of a {}\",\n",
        "        \"a photo of a nice {}\",\n",
        "        \"a good photo of a {}\",\n",
        "        \"a photo of the nice {}\",\n",
        "        \"a photo of the small {}\",\n",
        "        \"a photo of the weird {}\",\n",
        "        \"a photo of the large {}\",\n",
        "        \"a photo of a cool {}\",\n",
        "        \"a photo of a small {}\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIXa4LvqG9Fi"
      },
      "source": [
        "![https://i.imgur.com/gQCRjK6.png](https://i.imgur.com/gQCRjK6.png)\n",
        "\n",
        "Looks great!\n",
        "\n",
        "Next, we assemble a dataset of groups of our GitHub avatars:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TdUWTXrG9Fi"
      },
      "outputs": [],
      "source": [
        "group_ds = assemble_dataset(\n",
        "    urls=[\n",
        "        \"https://i.imgur.com/yVmZ2Qa.jpg\",\n",
        "        \"https://i.imgur.com/JbyFbZJ.jpg\",\n",
        "        \"https://i.imgur.com/CCubd3q.jpg\",\n",
        "    ],\n",
        "    prompts=[\n",
        "        \"a photo of a group of {}\",\n",
        "        \"a rendering of a group of {}\",\n",
        "        \"a cropped photo of the group of {}\",\n",
        "        \"the photo of a group of {}\",\n",
        "        \"a photo of a clean group of {}\",\n",
        "        \"a photo of my group of {}\",\n",
        "        \"a photo of a cool group of {}\",\n",
        "        \"a close-up photo of a group of {}\",\n",
        "        \"a bright photo of the group of {}\",\n",
        "        \"a cropped photo of a group of {}\",\n",
        "        \"a photo of the group of {}\",\n",
        "        \"a good photo of the group of {}\",\n",
        "        \"a photo of one group of {}\",\n",
        "        \"a close-up photo of the group of {}\",\n",
        "        \"a rendition of the group of {}\",\n",
        "        \"a photo of the clean group of {}\",\n",
        "        \"a rendition of a group of {}\",\n",
        "        \"a photo of a nice group of {}\",\n",
        "        \"a good photo of a group of {}\",\n",
        "        \"a photo of the nice group of {}\",\n",
        "        \"a photo of the small group of {}\",\n",
        "        \"a photo of the weird group of {}\",\n",
        "        \"a photo of the large group of {}\",\n",
        "        \"a photo of a cool group of {}\",\n",
        "        \"a photo of a small group of {}\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsNbt9rDG9Fj"
      },
      "source": [
        "![https://i.imgur.com/GY9Pf3D.png](https://i.imgur.com/GY9Pf3D.png)\n",
        "\n",
        "Finally, we concatenate the two datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Szjx6Fo-G9Fj"
      },
      "outputs": [],
      "source": [
        "train_ds = single_ds.concatenate(group_ds)\n",
        "train_ds = train_ds.batch(1).shuffle(\n",
        "    train_ds.cardinality(), reshuffle_each_iteration=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbKXhQUqG9Fj"
      },
      "source": [
        "## Adding a new token to the text encoder\n",
        "\n",
        "Next, we create a new text encoder for the StableDiffusion model and add our new\n",
        "embedding for '<my-funny-cat-token>' into the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrqOth_TG9Fk"
      },
      "outputs": [],
      "source": [
        "tokenized_initializer = stable_diffusion.tokenizer.encode(\"cat\")[1]\n",
        "new_weights = stable_diffusion.text_encoder.layers[2].token_embedding(\n",
        "    tf.constant(tokenized_initializer)\n",
        ")\n",
        "\n",
        "# Get len of .vocab instead of tokenizer\n",
        "new_vocab_size = len(stable_diffusion.tokenizer.vocab)\n",
        "\n",
        "# The embedding layer is the 2nd layer in the text encoder\n",
        "old_token_weights = stable_diffusion.text_encoder.layers[\n",
        "    2\n",
        "].token_embedding.get_weights()\n",
        "old_position_weights = stable_diffusion.text_encoder.layers[\n",
        "    2\n",
        "].position_embedding.get_weights()\n",
        "\n",
        "old_token_weights = old_token_weights[0]\n",
        "new_weights = np.expand_dims(new_weights, axis=0)\n",
        "new_weights = np.concatenate([old_token_weights, new_weights], axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS_NREr_G9Fk"
      },
      "source": [
        "Let's construct a new TextEncoder and prepare it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bm9MNsLG9Fk"
      },
      "outputs": [],
      "source": [
        "# Have to set download_weights False so we can init (otherwise tries to load weights)\n",
        "new_encoder = keras_cv.models.stable_diffusion.TextEncoder(\n",
        "    keras_cv.models.stable_diffusion.stable_diffusion.MAX_PROMPT_LENGTH,\n",
        "    vocab_size=new_vocab_size,\n",
        "    download_weights=False,\n",
        ")\n",
        "for index, layer in enumerate(stable_diffusion.text_encoder.layers):\n",
        "    # Layer 2 is the embedding layer, so we omit it from our weight-copying\n",
        "    if index == 2:\n",
        "        continue\n",
        "    new_encoder.layers[index].set_weights(layer.get_weights())\n",
        "\n",
        "\n",
        "new_encoder.layers[2].token_embedding.set_weights([new_weights])\n",
        "new_encoder.layers[2].position_embedding.set_weights(old_position_weights)\n",
        "\n",
        "stable_diffusion._text_encoder = new_encoder\n",
        "stable_diffusion._text_encoder.compile(jit_compile=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb0mfB6XG9Fk"
      },
      "source": [
        "## Training\n",
        "\n",
        "Now we can move on to the exciting part: training!\n",
        "\n",
        "In TextualInversion, the only piece of the model that is trained is the embedding vector.\n",
        "Let's freeze the rest of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6I_A08HSG9Fl"
      },
      "outputs": [],
      "source": [
        "\n",
        "stable_diffusion.diffusion_model.trainable = False\n",
        "stable_diffusion.decoder.trainable = False\n",
        "stable_diffusion.text_encoder.trainable = True\n",
        "\n",
        "stable_diffusion.text_encoder.layers[2].trainable = True\n",
        "\n",
        "\n",
        "def traverse_layers(layer):\n",
        "    if hasattr(layer, \"layers\"):\n",
        "        for layer in layer.layers:\n",
        "            yield layer\n",
        "    if hasattr(layer, \"token_embedding\"):\n",
        "        yield layer.token_embedding\n",
        "    if hasattr(layer, \"position_embedding\"):\n",
        "        yield layer.position_embedding\n",
        "\n",
        "\n",
        "for layer in traverse_layers(stable_diffusion.text_encoder):\n",
        "    if isinstance(layer, keras.layers.Embedding) or \"clip_embedding\" in layer.name:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False\n",
        "\n",
        "new_encoder.layers[2].position_embedding.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg8LziuuG9Fl"
      },
      "source": [
        "Let's confirm the proper weights are set to trainable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3Ow0W4YG9Fl"
      },
      "outputs": [],
      "source": [
        "all_models = [\n",
        "    stable_diffusion.text_encoder,\n",
        "    stable_diffusion.diffusion_model,\n",
        "    stable_diffusion.decoder,\n",
        "]\n",
        "print([[w.shape for w in model.trainable_weights] for model in all_models])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdrQsdZCG9Fl"
      },
      "source": [
        "## Training the new embedding\n",
        "\n",
        "In order to train the embedding, we need a couple of utilities.\n",
        "We import a NoiseScheduler from KerasCV, and define the following utilities below:\n",
        "\n",
        "- `sample_from_encoder_outputs` is a wrapper around the base StableDiffusion image\n",
        "encoder which samples from the statistical distribution produced by the image\n",
        "encoder, rather than taking just the mean (like many other SD applications)\n",
        "- `get_timestep_embedding` produces an embedding for a specified timestep for the\n",
        "diffusion model\n",
        "- `get_position_ids` produces a tensor of position IDs for the text encoder (which is just a\n",
        "series from `[1, MAX_PROMPT_LENGTH]`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PN-SA7GG9Fl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Remove the top layer from the encoder, which cuts off the variance and only returns\n",
        "# the mean\n",
        "training_image_encoder = keras.Model(\n",
        "    stable_diffusion.image_encoder.input,\n",
        "    stable_diffusion.image_encoder.layers[-2].output,\n",
        ")\n",
        "\n",
        "\n",
        "def sample_from_encoder_outputs(outputs):\n",
        "    mean, logvar = tf.split(outputs, 2, axis=-1)\n",
        "    logvar = tf.clip_by_value(logvar, -30.0, 20.0)\n",
        "    std = tf.exp(0.5 * logvar)\n",
        "    sample = tf.random.normal(tf.shape(mean))\n",
        "    return mean + std * sample\n",
        "\n",
        "\n",
        "def get_timestep_embedding(timestep, dim=320, max_period=10000):\n",
        "    half = dim // 2\n",
        "    freqs = tf.math.exp(\n",
        "        -math.log(max_period) * tf.range(0, half, dtype=tf.float32) / half\n",
        "    )\n",
        "    args = tf.convert_to_tensor([timestep], dtype=tf.float32) * freqs\n",
        "    embedding = tf.concat([tf.math.cos(args), tf.math.sin(args)], 0)\n",
        "    return embedding\n",
        "\n",
        "\n",
        "def get_position_ids():\n",
        "    return tf.convert_to_tensor([list(range(MAX_PROMPT_LENGTH))], dtype=tf.int32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xoFuJzSG9Fm"
      },
      "source": [
        "Next, we implement a `StableDiffusionFineTuner`, which is a subclass of `keras.Model`\n",
        "that overrides `train_step` to train the token embeddings of our text encoder.\n",
        "This is the core of the Textual Inversion algorithm.\n",
        "\n",
        "Abstractly speaking, the train step takes a sample from the output of the frozen SD\n",
        "image encoder's latent distribution for a training image, adds noise to that sample, and\n",
        "then passes that noisy sample to the frozen diffusion model.\n",
        "The hidden state of the diffusion model is the output of the text encoder for the prompt\n",
        "corresponding to the image.\n",
        "\n",
        "Our final goal state is that the diffusion model is able to separate the noise from the\n",
        "sample using the text encoding as hidden state, so our loss is the mean-squared error of\n",
        "the noise and the output of the diffusion model (which has, ideally, removed the image\n",
        "latents from the noise).\n",
        "\n",
        "We compute gradients for only the token embeddings of the text encoder, and in the\n",
        "train step we zero-out the gradients for all tokens other than the token that we're\n",
        "learning.\n",
        "\n",
        "See in-line code comments for more details about the train step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDqokMc7G9Fm"
      },
      "outputs": [],
      "source": [
        "\n",
        "class StableDiffusionFineTuner(keras.Model):\n",
        "    def __init__(self, stable_diffusion, noise_scheduler, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.stable_diffusion = stable_diffusion\n",
        "        self.noise_scheduler = noise_scheduler\n",
        "\n",
        "    def train_step(self, data):\n",
        "        images, embeddings = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Sample from the predicted distribution for the training image\n",
        "            latents = sample_from_encoder_outputs(training_image_encoder(images))\n",
        "            # The latents must be downsampled to match the scale of the latents used\n",
        "            # in the training of StableDiffusion.  This number is truly just a \"magic\"\n",
        "            # constant that they chose when training the model.\n",
        "            latents = latents * 0.18215\n",
        "\n",
        "            # Produce random noise in the same shape as the latent sample\n",
        "            noise = tf.random.normal(tf.shape(latents))\n",
        "            batch_dim = tf.shape(latents)[0]\n",
        "\n",
        "            # Pick a random timestep for each sample in the batch\n",
        "            timesteps = tf.random.uniform(\n",
        "                (batch_dim,),\n",
        "                minval=0,\n",
        "                maxval=noise_scheduler.train_timesteps,\n",
        "                dtype=tf.int64,\n",
        "            )\n",
        "\n",
        "            # Add noise to the latents based on the timestep for each sample\n",
        "            noisy_latents = self.noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "            # Encode the text in the training samples to use as hidden state in the\n",
        "            # diffusion model\n",
        "            encoder_hidden_state = self.stable_diffusion.text_encoder(\n",
        "                [embeddings, get_position_ids()]\n",
        "            )\n",
        "\n",
        "            # Compute timestep embeddings for the randomly-selected timesteps for each\n",
        "            # sample in the batch\n",
        "            timestep_embeddings = tf.map_fn(\n",
        "                fn=get_timestep_embedding,\n",
        "                elems=timesteps,\n",
        "                fn_output_signature=tf.float32,\n",
        "            )\n",
        "\n",
        "            # Call the diffusion model\n",
        "            noise_pred = self.stable_diffusion.diffusion_model(\n",
        "                [noisy_latents, timestep_embeddings, encoder_hidden_state]\n",
        "            )\n",
        "\n",
        "            # Compute the mean-squared error loss and reduce it.\n",
        "            loss = self.compiled_loss(noise_pred, noise)\n",
        "            loss = tf.reduce_mean(loss, axis=2)\n",
        "            loss = tf.reduce_mean(loss, axis=1)\n",
        "            loss = tf.reduce_mean(loss)\n",
        "\n",
        "        # Load the trainable weights and compute the gradients for them\n",
        "        trainable_weights = self.stable_diffusion.text_encoder.trainable_weights\n",
        "        grads = tape.gradient(loss, trainable_weights)\n",
        "\n",
        "        # Gradients are stored in indexed slices, so we have to find the index\n",
        "        # of the slice(s) which contain the placeholder token.\n",
        "        index_of_placeholder_token = tf.reshape(tf.where(grads[0].indices == 49408), ())\n",
        "        condition = grads[0].indices == 49408\n",
        "        condition = tf.expand_dims(condition, axis=-1)\n",
        "\n",
        "        # Override the gradients, zeroing out the gradients for all slices that\n",
        "        # aren't for the placeholder token, effectively freezing the weights for\n",
        "        # all other tokens.\n",
        "        grads[0] = tf.IndexedSlices(\n",
        "            values=tf.where(condition, grads[0].values, 0),\n",
        "            indices=grads[0].indices,\n",
        "            dense_shape=grads[0].dense_shape,\n",
        "        )\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(grads, trainable_weights))\n",
        "        return {\"loss\": loss}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejhYmDvnG9Fm"
      },
      "source": [
        "Before we start training, let's take a look at what StableDiffusion produces for our\n",
        "token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZss1rdGG9Fm"
      },
      "outputs": [],
      "source": [
        "generated = stable_diffusion.text_to_image(\n",
        "    f\"an oil painting of {placeholder_token}\", seed=1337, batch_size=3\n",
        ")\n",
        "plot_images(generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUPtpa8gG9Fn"
      },
      "source": [
        "As you can see, the model still thinks of our token as a cat, as this was the seed token\n",
        "we used to initialize our custom token.\n",
        "\n",
        "Now, to get started with training, we can just `compile()` our model like any other\n",
        "Keras model. Before doing so, we also instantiate a noise scheduler for training and\n",
        "configure our training parameters such as learning rate and optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MQelQ2bG9Fn"
      },
      "outputs": [],
      "source": [
        "noise_scheduler = NoiseScheduler(\n",
        "    beta_start=0.00085,\n",
        "    beta_end=0.012,\n",
        "    beta_schedule=\"scaled_linear\",\n",
        "    train_timesteps=1000,\n",
        ")\n",
        "trainer = StableDiffusionFineTuner(stable_diffusion, noise_scheduler, name=\"trainer\")\n",
        "EPOCHS = 50\n",
        "learning_rate = keras.optimizers.schedules.CosineDecay(\n",
        "    initial_learning_rate=1e-4, decay_steps=train_ds.cardinality() * EPOCHS\n",
        ")\n",
        "optimizer = keras.optimizers.Adam(\n",
        "    weight_decay=0.004, learning_rate=learning_rate, epsilon=1e-8, global_clipnorm=10\n",
        ")\n",
        "\n",
        "trainer.compile(\n",
        "    optimizer=optimizer,\n",
        "    # We are performing reduction manually in our train step, so none is required here.\n",
        "    loss=keras.losses.MeanSquaredError(reduction=\"none\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoAuAlKlG9Fn"
      },
      "source": [
        "To monitor training, we can produce a `keras.callbacks.Callback` to produce a few images\n",
        "every epoch using our custom token.\n",
        "\n",
        "We create three callbacks with different prompts so that we can see how they progress\n",
        "over the course of training. We use a fixed seed so that we can easily see the\n",
        "progression of the learned token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhcdVsYDG9Fn"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GenerateImages(keras.callbacks.Callback):\n",
        "    def __init__(\n",
        "        self, stable_diffusion, prompt, steps=50, frequency=10, seed=None, **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.stable_diffusion = stable_diffusion\n",
        "        self.prompt = prompt\n",
        "        self.seed = seed\n",
        "        self.frequency = frequency\n",
        "        self.steps = steps\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        if epoch % self.frequency == 0:\n",
        "            images = self.stable_diffusion.text_to_image(\n",
        "                self.prompt, batch_size=3, num_steps=self.steps, seed=self.seed\n",
        "            )\n",
        "            plot_images(\n",
        "                images,\n",
        "            )\n",
        "\n",
        "\n",
        "cbs = [\n",
        "    GenerateImages(\n",
        "        stable_diffusion, prompt=f\"an oil painting of {placeholder_token}\", seed=1337\n",
        "    ),\n",
        "    GenerateImages(\n",
        "        stable_diffusion, prompt=f\"gandalf the gray as a {placeholder_token}\", seed=1337\n",
        "    ),\n",
        "    GenerateImages(\n",
        "        stable_diffusion,\n",
        "        prompt=f\"two {placeholder_token} getting married, photorealistic, high quality\",\n",
        "        seed=1337,\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_rU_y6OG9Fo"
      },
      "source": [
        "Now, all that is left to do is to call `model.fit()`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAV-x_E3G9Fo"
      },
      "outputs": [],
      "source": [
        "trainer.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=cbs,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpJcROWZG9Fo"
      },
      "source": [
        "It's pretty fun to see how the model learns our new token over time. Play around with it\n",
        "and see how you can tune training parameters and your training dataset to produce the\n",
        "best images!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ktdAEoTG9Fo"
      },
      "source": [
        "## Taking the Fine Tuned Model for a Spin\n",
        "\n",
        "Now for the really fun part. We've learned a token embedding for our custom token, so\n",
        "now we can generate images with StableDiffusion the same way we would for any other\n",
        "token!\n",
        "\n",
        "Here are some fun example prompts to get you started, with sample outputs from our cat\n",
        "doll token!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdWskn5QG9Fp"
      },
      "outputs": [],
      "source": [
        "generated = stable_diffusion.text_to_image(\n",
        "    f\"Gandalf as a {placeholder_token} fantasy art drawn by disney concept artists, \"\n",
        "    \"golden colour, high quality, highly detailed, elegant, sharp focus, concept art, \"\n",
        "    \"character concepts, digital painting, mystery, adventure\",\n",
        "    batch_size=3,\n",
        ")\n",
        "plot_images(generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v04cMJeZG9Fp"
      },
      "outputs": [],
      "source": [
        "generated = stable_diffusion.text_to_image(\n",
        "    f\"A masterpiece of a {placeholder_token} crying out to the heavens. \"\n",
        "    f\"Behind the {placeholder_token}, an dark, evil shade looms over it - sucking the \"\n",
        "    \"life right out of it.\",\n",
        "    batch_size=3,\n",
        ")\n",
        "plot_images(generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBuyQq5pG9Fp"
      },
      "outputs": [],
      "source": [
        "generated = stable_diffusion.text_to_image(\n",
        "    f\"An evil {placeholder_token}.\", batch_size=3\n",
        ")\n",
        "plot_images(generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cu7XBnpsG9Fp"
      },
      "outputs": [],
      "source": [
        "generated = stable_diffusion.text_to_image(\n",
        "    f\"A mysterious {placeholder_token} approaches the great pyramids of egypt.\",\n",
        "    batch_size=3,\n",
        ")\n",
        "plot_images(generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VQAsD0jG9Fq"
      },
      "source": [
        "## Conclusions\n",
        "\n",
        "Using the Textual Inversion algorithm you can teach StableDiffusion new concepts!\n",
        "\n",
        "Some possible next steps to follow:\n",
        "\n",
        "- Try out your own prompts\n",
        "- Teach the model a style\n",
        "- Gather a dataset of your favorite pet cat or dog and teach the model about it"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "fine_tune_via_textual_inversion",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}